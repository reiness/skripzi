{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a2047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Setup\n",
    "import os\n",
    "import random\n",
    "import cv2  # OpenCV for video processing\n",
    "import time\n",
    "\n",
    "# For inference with YOLO (assuming you use ultralytics)\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define directory paths (modify as per your folder structure)\n",
    "test_videos_dir = 'data/test'  # Directory containing your test videos\n",
    "extracted_frames_dir = 'data/test_frames'  # Directory to save extracted frames\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "os.makedirs(extracted_frames_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "571fe46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos found: 10\n",
      "Selected candidate videos:\n",
      "bicycle_test.mp4\n",
      "car_test.mp4\n",
      "people_test.mp4\n"
     ]
    }
   ],
   "source": [
    "# select 3 random vids\n",
    "import random\n",
    "random.seed(162)\n",
    "\n",
    "# Cell 1: Randomly choose 3 candidate videos\n",
    "all_videos = [f for f in os.listdir(test_videos_dir) if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
    "print(f\"Total videos found: {len(all_videos)}\")\n",
    "\n",
    "# Select 3 random videos (ensure there are at least 3)\n",
    "num_candidates = 3\n",
    "candidate_videos = random.sample(all_videos, min(num_candidates, len(all_videos)))\n",
    "print(\"Selected candidate videos:\")\n",
    "for vid in candidate_videos:\n",
    "    print(vid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9621b17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: bicycle_test.mp4 | FPS: 28.00 | Duration: 79.11s\n",
      "Number of segments to extract: 11\n",
      "Saved frame at 3.0s (Frame 84) -> data/test_frames\\bicycle_test\\frame_0084.jpg\n",
      "Saved frame at 10.0s (Frame 280) -> data/test_frames\\bicycle_test\\frame_0280.jpg\n",
      "Saved frame at 17.0s (Frame 476) -> data/test_frames\\bicycle_test\\frame_0476.jpg\n",
      "Saved frame at 24.0s (Frame 672) -> data/test_frames\\bicycle_test\\frame_0672.jpg\n",
      "Saved frame at 31.0s (Frame 868) -> data/test_frames\\bicycle_test\\frame_0868.jpg\n",
      "Saved frame at 38.0s (Frame 1064) -> data/test_frames\\bicycle_test\\frame_1064.jpg\n",
      "Saved frame at 45.0s (Frame 1260) -> data/test_frames\\bicycle_test\\frame_1260.jpg\n",
      "Saved frame at 52.0s (Frame 1456) -> data/test_frames\\bicycle_test\\frame_1456.jpg\n",
      "Saved frame at 59.0s (Frame 1652) -> data/test_frames\\bicycle_test\\frame_1652.jpg\n",
      "Saved frame at 66.0s (Frame 1848) -> data/test_frames\\bicycle_test\\frame_1848.jpg\n",
      "Saved frame at 73.0s (Frame 2044) -> data/test_frames\\bicycle_test\\frame_2044.jpg\n",
      "\n",
      "Video: car_test.mp4 | FPS: 29.97 | Duration: 210.91s\n",
      "Number of segments to extract: 30\n",
      "Saved frame at 3.0s (Frame 89) -> data/test_frames\\car_test\\frame_0089.jpg\n",
      "Saved frame at 10.0s (Frame 299) -> data/test_frames\\car_test\\frame_0299.jpg\n",
      "Saved frame at 17.0s (Frame 509) -> data/test_frames\\car_test\\frame_0509.jpg\n",
      "Saved frame at 24.0s (Frame 719) -> data/test_frames\\car_test\\frame_0719.jpg\n",
      "Saved frame at 31.0s (Frame 929) -> data/test_frames\\car_test\\frame_0929.jpg\n",
      "Saved frame at 38.0s (Frame 1138) -> data/test_frames\\car_test\\frame_1138.jpg\n",
      "Saved frame at 45.0s (Frame 1348) -> data/test_frames\\car_test\\frame_1348.jpg\n",
      "Saved frame at 52.0s (Frame 1558) -> data/test_frames\\car_test\\frame_1558.jpg\n",
      "Saved frame at 59.0s (Frame 1768) -> data/test_frames\\car_test\\frame_1768.jpg\n",
      "Saved frame at 66.0s (Frame 1978) -> data/test_frames\\car_test\\frame_1978.jpg\n",
      "Saved frame at 73.0s (Frame 2187) -> data/test_frames\\car_test\\frame_2187.jpg\n",
      "Saved frame at 80.0s (Frame 2397) -> data/test_frames\\car_test\\frame_2397.jpg\n",
      "Saved frame at 87.0s (Frame 2607) -> data/test_frames\\car_test\\frame_2607.jpg\n",
      "Saved frame at 94.0s (Frame 2817) -> data/test_frames\\car_test\\frame_2817.jpg\n",
      "Saved frame at 101.0s (Frame 3026) -> data/test_frames\\car_test\\frame_3026.jpg\n",
      "Saved frame at 108.0s (Frame 3236) -> data/test_frames\\car_test\\frame_3236.jpg\n",
      "Saved frame at 115.0s (Frame 3446) -> data/test_frames\\car_test\\frame_3446.jpg\n",
      "Saved frame at 122.0s (Frame 3656) -> data/test_frames\\car_test\\frame_3656.jpg\n",
      "Saved frame at 129.0s (Frame 3866) -> data/test_frames\\car_test\\frame_3866.jpg\n",
      "Saved frame at 136.0s (Frame 4075) -> data/test_frames\\car_test\\frame_4075.jpg\n",
      "Saved frame at 143.0s (Frame 4285) -> data/test_frames\\car_test\\frame_4285.jpg\n",
      "Saved frame at 150.0s (Frame 4495) -> data/test_frames\\car_test\\frame_4495.jpg\n",
      "Saved frame at 157.0s (Frame 4705) -> data/test_frames\\car_test\\frame_4705.jpg\n",
      "Saved frame at 164.0s (Frame 4915) -> data/test_frames\\car_test\\frame_4915.jpg\n",
      "Saved frame at 171.0s (Frame 5124) -> data/test_frames\\car_test\\frame_5124.jpg\n",
      "Saved frame at 178.0s (Frame 5334) -> data/test_frames\\car_test\\frame_5334.jpg\n",
      "Saved frame at 185.0s (Frame 5544) -> data/test_frames\\car_test\\frame_5544.jpg\n",
      "Saved frame at 192.0s (Frame 5754) -> data/test_frames\\car_test\\frame_5754.jpg\n",
      "Saved frame at 199.0s (Frame 5964) -> data/test_frames\\car_test\\frame_5964.jpg\n",
      "Saved frame at 206.0s (Frame 6173) -> data/test_frames\\car_test\\frame_6173.jpg\n",
      "\n",
      "Video: people_test.mp4 | FPS: 29.87 | Duration: 95.94s\n",
      "Number of segments to extract: 13\n",
      "Saved frame at 3.0s (Frame 89) -> data/test_frames\\people_test\\frame_0089.jpg\n",
      "Saved frame at 10.0s (Frame 298) -> data/test_frames\\people_test\\frame_0298.jpg\n",
      "Saved frame at 17.0s (Frame 507) -> data/test_frames\\people_test\\frame_0507.jpg\n",
      "Saved frame at 24.0s (Frame 716) -> data/test_frames\\people_test\\frame_0716.jpg\n",
      "Saved frame at 31.0s (Frame 926) -> data/test_frames\\people_test\\frame_0926.jpg\n",
      "Saved frame at 38.0s (Frame 1135) -> data/test_frames\\people_test\\frame_1135.jpg\n",
      "Saved frame at 45.0s (Frame 1344) -> data/test_frames\\people_test\\frame_1344.jpg\n",
      "Saved frame at 52.0s (Frame 1553) -> data/test_frames\\people_test\\frame_1553.jpg\n",
      "Saved frame at 59.0s (Frame 1762) -> data/test_frames\\people_test\\frame_1762.jpg\n",
      "Saved frame at 66.0s (Frame 1971) -> data/test_frames\\people_test\\frame_1971.jpg\n",
      "Saved frame at 73.0s (Frame 2180) -> data/test_frames\\people_test\\frame_2180.jpg\n",
      "Saved frame at 80.0s (Frame 2389) -> data/test_frames\\people_test\\frame_2389.jpg\n",
      "Saved frame at 87.0s (Frame 2599) -> data/test_frames\\people_test\\frame_2599.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Extract one frame from each segment\n",
    "# Define the segment length and the gap between segments\n",
    "segment_length = 6   # seconds per segment\n",
    "segment_gap = 1      # gap between segments, so segments start at 0, 7, 13, etc.\n",
    "\n",
    "# We'll choose the middle of each segment. For a 6-sec segment, the midpoint is 3 seconds after the segment's start.\n",
    "def extract_segment_frames(video_path, output_dir, seg_length, seg_gap):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get FPS and total frame count\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration_sec = total_frames / video_fps\n",
    "    print(f\"Video: {os.path.basename(video_path)} | FPS: {video_fps:.2f} | Duration: {duration_sec:.2f}s\")\n",
    "    \n",
    "    # Calculate the start times for segments; segments start at times 0, (seg_length + seg_gap), (2*(seg_length+seg_gap)), etc.\n",
    "    segment_interval = seg_length + seg_gap\n",
    "    segment_starts = [t for t in range(0, int(duration_sec), segment_interval) if t + seg_length <= duration_sec]\n",
    "    \n",
    "    print(f\"Number of segments to extract: {len(segment_starts)}\")\n",
    "    \n",
    "    # For each segment, choose the midpoint frame (start time + seg_length/2)\n",
    "    for seg_start in segment_starts:\n",
    "        target_time = seg_start + seg_length / 2  # in seconds\n",
    "        target_frame_index = int(target_time * video_fps)\n",
    "        \n",
    "        # Set the video capture position to the target frame index\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame_index)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Define filename and save the extracted frame\n",
    "            frame_filename = os.path.join(output_dir, f\"frame_{target_frame_index:04d}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            print(f\"Saved frame at {target_time:.1f}s (Frame {target_frame_index}) -> {frame_filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to capture frame at {target_time:.1f}s (Frame {target_frame_index})\")\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "# Process each candidate video\n",
    "for vid_name in candidate_videos:\n",
    "    vid_path = os.path.join(test_videos_dir, vid_name)\n",
    "    # Create a subdirectory for frames for this video\n",
    "    video_frames_dir = os.path.join(extracted_frames_dir, os.path.splitext(vid_name)[0])\n",
    "    os.makedirs(video_frames_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract frames with non-continuous segments\n",
    "    extract_segment_frames(vid_path, video_frames_dir, segment_length, segment_gap)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa28f0",
   "metadata": {},
   "source": [
    "# TEST CLASS CONVERTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d69f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# names: ['Bicycle', 'Car', 'Motorbike', 'People']\n",
    "\n",
    "# ACTUAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b948580",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe215ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Cell 4: Run inference on all images in a folder, measure inference time, and calculate mAP\n",
    "\n",
    "\n",
    "# Load your pre-trained YOLO model (update the model path accordingly)\n",
    "model = YOLO(\"C:\\\\Users\\\\redoks\\\\Documents\\\\skripzii\\\\products6k\\\\YOLOv9\\\\YOLOv9s\\\\batch8_lr0.01\\\\weights\\\\best.pt\")  # Replace with your model file\n",
    "\n",
    "def run_inference_on_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    start_time = time.time()\n",
    "    # Run prediction; here we use conf=0.25 as default\n",
    "    results = model.predict(source=img, conf=0.489, imgsz=640)\n",
    "    elapsed = time.time() - start_time\n",
    "    return results, elapsed\n",
    "\n",
    "def parse_results(results):\n",
    "    \"\"\"Extract bounding boxes and classes from YOLO results.\"\"\"\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()  # Bounding boxes (x1, y1, x2, y2)\n",
    "    scores = results[0].boxes.conf.cpu().numpy()  # Confidence scores\n",
    "    classes = results[0].boxes.cls.cpu().numpy()  # Class IDs\n",
    "    return boxes, scores, classes\n",
    "\n",
    "def parse_ground_truth(label_path):\n",
    "    \"\"\"Parse ground truth labels from a YOLO-format label file.\"\"\"\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    gt_boxes = []\n",
    "    gt_classes = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        cls = int(parts[0])  # Class ID\n",
    "        x_center, y_center, width, height = map(float, parts[1:])\n",
    "        gt_classes.append(cls)\n",
    "        # Convert YOLO format (x_center, y_center, width, height) to (x1, y1, x2, y2)\n",
    "        x1 = x_center - width / 2\n",
    "        y1 = y_center - height / 2\n",
    "        x2 = x_center + width / 2\n",
    "        y2 = y_center + height / 2\n",
    "        gt_boxes.append([x1, y1, x2, y2])\n",
    "    return gt_boxes, gt_classes\n",
    "\n",
    "# Define the folder containing images and labels\n",
    "images_dir = 'data/test_ready/test/images'\n",
    "labels_dir = 'data/test_ready/test/labels'\n",
    "\n",
    "# Get all image files in the folder\n",
    "image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "print(f\"Found {len(image_files)} images in {images_dir}\")\n",
    "\n",
    "# Run inference on all images\n",
    "all_inference_times = []\n",
    "all_aps = []  # Store average precision for each image\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(images_dir, image_file)\n",
    "    label_path = os.path.join(labels_dir, os.path.splitext(image_file)[0] + '.txt')  # Corresponding label file\n",
    "\n",
    "    # Check if the label file exists\n",
    "    if not os.path.exists(label_path):\n",
    "        print(f\"Warning: Label file not found for {image_file}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Run inference\n",
    "    results, elapsed = run_inference_on_image(image_path)\n",
    "    all_inference_times.append(elapsed)\n",
    "\n",
    "    # Parse results and ground truth\n",
    "    pred_boxes, pred_scores, pred_classes = parse_results(results)\n",
    "    gt_boxes, gt_classes = parse_ground_truth(label_path)\n",
    "\n",
    "    # Calculate mAP (for simplicity, we calculate AP per class and average them)\n",
    "    aps = []\n",
    "    for cls in set(gt_classes + list(pred_classes)):\n",
    "        gt_binary = [1 if c == cls else 0 for c in gt_classes]\n",
    "        # Filter pred_scores for predictions matching the current class:\n",
    "        pred_scores_cls = [score for score, c in zip(pred_scores, pred_classes) if c == cls]\n",
    "        # Also create a pred_binary that has the same length as pred_scores_cls (typically all ones, since these are only predictions for this class)\n",
    "        pred_binary = [1] * len(pred_scores_cls)\n",
    "\n",
    "        if sum(gt_binary) > 0 and len(pred_scores_cls) > 0:\n",
    "            aps.append(average_precision_score(gt_binary, pred_scores_cls))\n",
    "\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Image {image_file}: Inference time: {elapsed*1000:.2f} ms\")\n",
    "    print(f\"Results: {results}\")  # You can process results further if needed\n",
    "\n",
    "# Calculate and display average inference time and mAP\n",
    "if all_inference_times:\n",
    "    avg_time = sum(all_inference_times) / len(all_inference_times)\n",
    "    print(f\"Average inference time per image: {avg_time*1000:.2f} ms\")\n",
    "\n",
    "if all_aps:\n",
    "    mean_ap = sum(all_aps) / len(all_aps)\n",
    "    print(f\"Mean Average Precision (mAP): {mean_ap:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101bb19d",
   "metadata": {},
   "source": [
    "# 2nd way to inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471a5300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.107  Python-3.12.0 torch-2.6.0+cpu CPU (Intel Core(TM) i5-8265U 1.60GHz)\n",
      "YOLOv9s summary (fused): 197 layers, 7,171,732 parameters, 0 gradients, 26.8 GFLOPs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "\nDataset 'C://Users/redoks/Documents/skripzii/data/test_ready/data.yaml' images not found ⚠️, missing path 'C:\\Users\\redoks\\Documents\\skripzii\\data\\test_ready\\valid\\images'\nNote dataset download directory is 'C:\\Users\\redoks\\Documents\\skripzii\\datasets'. You can update this in 'C:\\Users\\redoks\\AppData\\Roaming\\Ultralytics\\settings.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# === Run validation using the test set ===\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mredoks\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDocuments\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mskripzii\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtest_ready\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Force it to use the test split\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.489\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Confidence threshold\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43miou\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# IoU threshold\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Image size (default from YOLO config)\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_det\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Max detections per image\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Set to 0 for GPU, or 'cpu' if needed\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# === End timer ===\u001b[39;00m\n\u001b[0;32m     22\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\redoks\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:628\u001b[0m, in \u001b[0;36mModel.val\u001b[1;34m(self, validator, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[0;32m    627\u001b[0m validator \u001b[38;5;241m=\u001b[39m (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidator\u001b[39m\u001b[38;5;124m\"\u001b[39m))(args\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 628\u001b[0m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m validator\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[1;32mc:\\Users\\redoks\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\redoks\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\validator.py:179\u001b[0m, in \u001b[0;36mBaseValidator.__call__\u001b[1;34m(self, trainer, model)\u001b[0m\n\u001b[0;32m    176\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting batch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input of shape (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 3, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgsz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgsz\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myml\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m check_cls_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msplit)\n",
      "File \u001b[1;32mc:\\Users\\redoks\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\data\\utils.py:372\u001b[0m, in \u001b[0;36mcheck_det_dataset\u001b[1;34m(dataset, autodownload)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    371\u001b[0m     m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote dataset download directory is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASETS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You can update this in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSETTINGS_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(m)\n\u001b[0;32m    373\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    374\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# success\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: \nDataset 'C://Users/redoks/Documents/skripzii/data/test_ready/data.yaml' images not found ⚠️, missing path 'C:\\Users\\redoks\\Documents\\skripzii\\data\\test_ready\\valid\\images'\nNote dataset download directory is 'C:\\Users\\redoks\\Documents\\skripzii\\datasets'. You can update this in 'C:\\Users\\redoks\\AppData\\Roaming\\Ultralytics\\settings.json'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# === Load your trained model ===\n",
    "model = YOLO(\"C:\\\\Users\\\\redoks\\\\Documents\\\\skripzii\\\\products6k\\\\YOLOv9\\\\YOLOv9s\\\\batch8_lr0.01\\\\weights\\\\best.pt\")  # Replace with your model file\n",
    "\n",
    "# === Start timer for full validation phase ===\n",
    "start_time = time.time()\n",
    "\n",
    "# === Run validation using the test set ===\n",
    "results = model.val(\n",
    "    data=r\"C:\\Users\\redoks\\Documents\\skripzii\\data\\test_ready\\data.yaml\",\n",
    "    split='test',       # Force it to use the test split\n",
    "    conf=0.489,           # Confidence threshold\n",
    "    iou=0.5,             # IoU threshold\n",
    "    imgsz=640,           # Image size (default from YOLO config)\n",
    "    max_det=300,         # Max detections per image\n",
    "    device='cpu'           # Set to 0 for GPU, or 'cpu' if needed\n",
    ")\n",
    "\n",
    "# === End timer ===\n",
    "end_time = time.time()\n",
    "\n",
    "# === Print relevant metrics ===\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(f\"mAP@0.5:        {results.box.map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95:   {results.box.map:.4f}\")\n",
    "print(f\"Inference Time: {results.speed['inference']:.2f} ms/image\")\n",
    "print(f\"Total Time:     {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1125a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: (Optional) Evaluate mAP using the YOLO test command (run in a terminal or via subprocess)\n",
    "!yolo task=detect mode=test model=path/to/your/trained_model.pt data=path/to/dataset.yaml imgsz=640 conf=0.25\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
