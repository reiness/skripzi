{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6a2047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Setup\n",
    "import os\n",
    "import random\n",
    "import cv2  # OpenCV for video processing\n",
    "import time\n",
    "\n",
    "# For inference with YOLO (assuming you use ultralytics)\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define directory paths (modify as per your folder structure)\n",
    "test_videos_dir = 'data/test_raw'  # Directory containing your test videos\n",
    "extracted_frames_dir = 'data/test_frames3'  # Directory to save extracted frames\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "os.makedirs(extracted_frames_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571fe46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos found: 10\n",
      "Selected candidate videos:\n",
      "bicycle_test.mp4\n",
      "car_test.mp4\n",
      "people_test.mp4\n"
     ]
    }
   ],
   "source": [
    "# select 3 random vids\n",
    "import random\n",
    "random.seed(162)\n",
    "\n",
    "# Cell 1: Randomly choose 3 candidate videos\n",
    "all_videos = [f for f in os.listdir(test_videos_dir) if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
    "print(f\"Total videos found: {len(all_videos)}\")\n",
    "\n",
    "# Select 3 random videos (ensure there are at least 3)\n",
    "num_candidates = 3\n",
    "candidate_videos = random.sample(all_videos, min(num_candidates, len(all_videos)))\n",
    "print(\"Selected candidate videos:\")\n",
    "for vid in candidate_videos:\n",
    "    print(vid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ab5e945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos found: 6\n"
     ]
    }
   ],
   "source": [
    "# use all vids for inference\n",
    "import os\n",
    "candidate_videos = []\n",
    "\n",
    "test_raw_dir = r'C:\\Users\\redoks\\Documents\\skripzii\\data\\test_raw'\n",
    "candidate_videos = [f for f in os.listdir(test_raw_dir) if f.lower().endswith('.mp4')]\n",
    "candidate_videos = [vid for vid in candidate_videos if vid not in ['bus_test.mp4', 'boat_test.mp4', 'dog-chair-bottle_test.mp4','table_test.mp4']]\n",
    "print(f\"Total videos found: {len(candidate_videos)}\")\n",
    "\n",
    "special_candidate = ['bus_test.mp4', 'boat_test.mp4', 'dog-chair-bottle_test.mp4']\n",
    "fastframe_candidate = ['table_test.mp4']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621b17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: table_test.mp4 | FPS: 25.00 | Duration: 14.60s\n",
      "Number of segments to extract: 7\n",
      "Saved frame at 0.5s (Frame 12) -> data/test_frames3\\table_test\\frame_0012.jpg\n",
      "Saved frame at 2.5s (Frame 62) -> data/test_frames3\\table_test\\frame_0062.jpg\n",
      "Saved frame at 4.5s (Frame 112) -> data/test_frames3\\table_test\\frame_0112.jpg\n",
      "Saved frame at 6.5s (Frame 162) -> data/test_frames3\\table_test\\frame_0162.jpg\n",
      "Saved frame at 8.5s (Frame 212) -> data/test_frames3\\table_test\\frame_0212.jpg\n",
      "Saved frame at 10.5s (Frame 262) -> data/test_frames3\\table_test\\frame_0262.jpg\n",
      "Saved frame at 12.5s (Frame 312) -> data/test_frames3\\table_test\\frame_0312.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(162)\n",
    "\n",
    "# Cell 2: Extract one frame from each segment\n",
    "# Define the segment length and the gap between segments\n",
    "segment_length = 6   # seconds per segment\n",
    "segment_gap = 1      # gap between segments, so segments start at 0, 7, 13, etc.\n",
    "special_length = 2 # for special candidates (bus and boat)\n",
    "fast_length = 1 # for fast frame candidates (table)\n",
    "\n",
    "# We'll choose the middle of each segment. For a 6-sec segment, the midpoint is 3 seconds after the segment's start.\n",
    "def extract_segment_frames(video_path, output_dir, seg_length, seg_gap):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get FPS and total frame count\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration_sec = total_frames / video_fps\n",
    "    print(f\"Video: {os.path.basename(video_path)} | FPS: {video_fps:.2f} | Duration: {duration_sec:.2f}s\")\n",
    "    \n",
    "    # Calculate the start times for segments; segments start at times 0, (seg_length + seg_gap), (2*(seg_length+seg_gap)), etc.\n",
    "    segment_interval = seg_length + seg_gap\n",
    "    segment_starts = [t for t in range(0, int(duration_sec), segment_interval) if t + seg_length <= duration_sec]\n",
    "    \n",
    "    print(f\"Number of segments to extract: {len(segment_starts)}\")\n",
    "    \n",
    "    # For each segment, choose the midpoint frame (start time + seg_length/2)\n",
    "    for seg_start in segment_starts:\n",
    "        target_time = seg_start + seg_length / 2  # in seconds\n",
    "        target_frame_index = int(target_time * video_fps)\n",
    "        \n",
    "        # Set the video capture position to the target frame index\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame_index)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Define filename and save the extracted frame\n",
    "            frame_filename = os.path.join(output_dir, f\"frame_{target_frame_index:04d}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            print(f\"Saved frame at {target_time:.1f}s (Frame {target_frame_index}) -> {frame_filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to capture frame at {target_time:.1f}s (Frame {target_frame_index})\")\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "# Process each candidate video\n",
    "for vid_name in candidate_videos:\n",
    "    vid_path = os.path.join(test_videos_dir, vid_name)\n",
    "    # Create a subdirectory for frames for this video\n",
    "    video_frames_dir = os.path.join(extracted_frames_dir, os.path.splitext(vid_name)[0])\n",
    "    os.makedirs(video_frames_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract frames with non-continuous segments\n",
    "    extract_segment_frames(vid_path, video_frames_dir, segment_length, segment_gap)\n",
    "    print()\n",
    "\n",
    "# Process each candidate video\n",
    "for vid_name in special_candidate:\n",
    "    vid_path = os.path.join(test_videos_dir, vid_name)\n",
    "    # Create a subdirectory for frames for this video\n",
    "    video_frames_dir = os.path.join(extracted_frames_dir, os.path.splitext(vid_name)[0])\n",
    "    os.makedirs(video_frames_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract frames with non-continuous segments\n",
    "    extract_segment_frames(vid_path, video_frames_dir, special_length, segment_gap)\n",
    "    print()\n",
    "\n",
    "# Process each candidate video\n",
    "for vid_name in fastframe_candidate:\n",
    "    vid_path = os.path.join(test_videos_dir, vid_name)\n",
    "    # Create a subdirectory for frames for this video\n",
    "    video_frames_dir = os.path.join(extracted_frames_dir, os.path.splitext(vid_name)[0])\n",
    "    os.makedirs(video_frames_dir, exist_ok=True)\n",
    "    \n",
    "    # Extract frames with non-continuous segments\n",
    "    extract_segment_frames(vid_path, video_frames_dir, fast_length, segment_gap)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa28f0",
   "metadata": {},
   "source": [
    "# TEST CLASS CONVERTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d69f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# names: ['Bicycle', 'Car', 'Motorbike', 'People']\n",
    "# 0, 1, 2, 3\n",
    "\n",
    "# ACTUAL\n",
    "#names: ['Bicycle', 'Boat', 'Bottle', 'Bus', 'Car', 'Cat', 'Chair', 'Cup', 'Dog', 'Motorbike', 'People', 'Table']\n",
    "# 0, 4, 9, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b948580",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe215ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Cell 4: Run inference on all images in a folder, measure inference time, and calculate mAP\n",
    "\n",
    "\n",
    "# Load your pre-trained YOLO model (update the model path accordingly)\n",
    "model = YOLO(\"C:\\\\Users\\\\redoks\\\\Documents\\\\skripzii\\\\products6k\\\\YOLOv9\\\\YOLOv9s\\\\batch8_lr0.01\\\\weights\\\\best.pt\")  # Replace with your model file\n",
    "\n",
    "def run_inference_on_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    start_time = time.time()\n",
    "    # Run prediction; here we use conf=0.25 as default\n",
    "    results = model.predict(source=img, conf=0.489, imgsz=640)\n",
    "    elapsed = time.time() - start_time\n",
    "    return results, elapsed\n",
    "\n",
    "def parse_results(results):\n",
    "    \"\"\"Extract bounding boxes and classes from YOLO results.\"\"\"\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()  # Bounding boxes (x1, y1, x2, y2)\n",
    "    scores = results[0].boxes.conf.cpu().numpy()  # Confidence scores\n",
    "    classes = results[0].boxes.cls.cpu().numpy()  # Class IDs\n",
    "    return boxes, scores, classes\n",
    "\n",
    "def parse_ground_truth(label_path):\n",
    "    \"\"\"Parse ground truth labels from a YOLO-format label file.\"\"\"\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    gt_boxes = []\n",
    "    gt_classes = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        cls = int(parts[0])  # Class ID\n",
    "        x_center, y_center, width, height = map(float, parts[1:])\n",
    "        gt_classes.append(cls)\n",
    "        # Convert YOLO format (x_center, y_center, width, height) to (x1, y1, x2, y2)\n",
    "        x1 = x_center - width / 2\n",
    "        y1 = y_center - height / 2\n",
    "        x2 = x_center + width / 2\n",
    "        y2 = y_center + height / 2\n",
    "        gt_boxes.append([x1, y1, x2, y2])\n",
    "    return gt_boxes, gt_classes\n",
    "\n",
    "# Define the folder containing images and labels\n",
    "images_dir = 'data/test_ready/test/images'\n",
    "labels_dir = 'data/test_ready/test/labels'\n",
    "\n",
    "# Get all image files in the folder\n",
    "image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "print(f\"Found {len(image_files)} images in {images_dir}\")\n",
    "\n",
    "# Run inference on all images\n",
    "all_inference_times = []\n",
    "all_aps = []  # Store average precision for each image\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(images_dir, image_file)\n",
    "    label_path = os.path.join(labels_dir, os.path.splitext(image_file)[0] + '.txt')  # Corresponding label file\n",
    "    # print(image_path)\n",
    "    # print(label_path)\n",
    "    # exit()\n",
    "\n",
    "    # Check if the label file exists\n",
    "    if not os.path.exists(label_path):\n",
    "        print(f\"Warning: Label file not found for {image_file}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Run inference\n",
    "    results, elapsed = run_inference_on_image(image_path)\n",
    "    all_inference_times.append(elapsed)\n",
    "\n",
    "    # Parse results and ground truth\n",
    "    pred_boxes, pred_scores, pred_classes = parse_results(results)\n",
    "    gt_boxes, gt_classes = parse_ground_truth(label_path)\n",
    "\n",
    "    # Calculate mAP (for simplicity, we calculate AP per class and average them)\n",
    "    aps = []\n",
    "    for cls in set(gt_classes + list(pred_classes)):\n",
    "        gt_binary = [1 if c == cls else 0 for c in gt_classes]\n",
    "        # Filter pred_scores for predictions matching the current class:\n",
    "        pred_scores_cls = [score for score, c in zip(pred_scores, pred_classes) if c == cls]\n",
    "        # Also create a pred_binary that has the same length as pred_scores_cls (typically all ones, since these are only predictions for this class)\n",
    "        pred_binary = [1] * len(pred_scores_cls)\n",
    "\n",
    "        if sum(gt_binary) > 0 and len(pred_scores_cls) > 0:\n",
    "            aps.append(average_precision_score(gt_binary, pred_scores_cls))\n",
    "\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Image {image_file}: Inference time: {elapsed*1000:.2f} ms\")\n",
    "    print(f\"Results: {results}\")  # You can process results further if needed\n",
    "\n",
    "# Calculate and display average inference time and mAP\n",
    "if all_inference_times:\n",
    "    avg_time = sum(all_inference_times) / len(all_inference_times)\n",
    "    print(f\"Average inference time per image: {avg_time*1000:.2f} ms\")\n",
    "\n",
    "if all_aps:\n",
    "    mean_ap = sum(all_aps) / len(all_aps)\n",
    "    print(f\"Mean Average Precision (mAP): {mean_ap:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101bb19d",
   "metadata": {},
   "source": [
    "# 2nd way to inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "471a5300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.107  Python-3.12.0 torch-2.6.0+cpu CPU (Intel Core(TM) i5-8265U 1.60GHz)\n",
      "YOLOv9s summary (fused): 197 layers, 7,171,732 parameters, 0 gradients, 26.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\redoks\\Documents\\skripzii\\data\\test_ready2\\test\\labels... 56 images, 0 backgrounds, 0 corrupt: 100%|██████████| 56/56 [00:00<00:00, 468.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\redoks\\Documents\\skripzii\\data\\test_ready2\\test\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:13<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        533      0.535      0.233      0.396      0.181\n",
      "               Bicycle         12         21      0.833      0.238      0.555      0.108\n",
      "                  Boat          7         12        0.4      0.167      0.284       0.16\n",
      "                Bottle         14         26        0.4     0.0769      0.215     0.0648\n",
      "                   Bus          3          3      0.333      0.333      0.446      0.223\n",
      "                   Car          8         14        0.4      0.286      0.414      0.231\n",
      "                   Cat          9         23      0.714      0.217      0.467      0.214\n",
      "                 Chair         19        119      0.614      0.294      0.431      0.249\n",
      "                   Cup          6        114          0          0          0          0\n",
      "                   Dog          6         12        0.6       0.25      0.473       0.24\n",
      "             Motorbike          6         18      0.714      0.278      0.489      0.208\n",
      "                People         28        103      0.949      0.359      0.657       0.32\n",
      "                 Table         23         68      0.465      0.294       0.32      0.152\n",
      "Speed: 1.3ms preprocess, 199.4ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val19\u001b[0m\n",
      "\n",
      "=== Evaluation Results ===\n",
      "mAP@0.5:        0.3960\n",
      "mAP@0.5:0.95:   0.1807\n",
      "Inference Time: 199.39 ms/image\n",
      "Total Time:     33.51 seconds\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# === Load your trained model ===\n",
    "model = YOLO(\"C:\\\\Users\\\\redoks\\\\Documents\\\\skripzii\\\\products6k\\\\YOLOv9\\\\YOLOv9s\\\\batch8_lr0.01\\\\weights\\\\best.pt\")  # Replace with your model file\n",
    "\n",
    "# === Start timer for full validation phase ===\n",
    "start_time = time.time()\n",
    "\n",
    "# === Run validation using the test set ===\n",
    "results = model.val(\n",
    "    data=r'C:\\Users\\redoks\\Documents\\skripzii\\data\\test_ready2\\data.yaml',\n",
    "    split='test',       # Force it to use the test split\n",
    "    conf=0.489,           # Confidence threshold\n",
    "    iou=0.5,             # IoU threshold\n",
    "    imgsz=640,           # Image size (default from YOLO config)\n",
    "    # max_det=300,         # Max detections per image\n",
    "    device='cpu',\n",
    "    cache=False,\n",
    ")\n",
    "\n",
    "# === End timer ===\n",
    "end_time = time.time()\n",
    "\n",
    "# === Print relevant metrics ===\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(f\"mAP@0.5:        {results.box.map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95:   {results.box.map:.4f}\")\n",
    "print(f\"Inference Time: {results.speed['inference']:.2f} ms/image\")\n",
    "print(f\"Total Time:     {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e87fce",
   "metadata": {},
   "source": [
    "# 3rd way inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|██████████| 18/18 [00:05<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[          0           0           0           0           0           0           0           0           0           0           0           0          37]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [          0           0           0           0           0           0           0           0           0           0           0           0           0]\n",
      " [         21           0           0           0           0           0           0           0           0           0           0           0           0]]\n",
      "\n",
      "=== Evaluation on Classes [0, 4, 9, 10] ===\n",
      "Class 0 (Bicycle): Precision 0.0000, Recall 0.0000\n",
      "Class 4 (Car): Precision 0.0000, Recall 0.0000\n",
      "Class 9 (Motorbike): Precision 0.0000, Recall 0.0000\n",
      "Class 10 (People): Precision 0.0000, Recall 0.0000\n",
      "Inference Time: 326.97 ms/image\n",
      "Total Time:     5.89 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from ultralytics.utils.metrics import ConfusionMatrix\n",
    "from ultralytics.utils.ops import xywh2xyxy\n",
    "\n",
    "# === Load full model (trained with 12 classes) ===\n",
    "model = YOLO(\"C:\\\\Users\\\\redoks\\\\Documents\\\\skripzii\\\\products6k\\\\YOLOv9\\\\YOLOv9s\\\\batch8_lr0.01\\\\weights\\\\best.pt\")  # Replace with your model file\n",
    "\n",
    "# === Define only test classes to evaluate ===\n",
    "selected_classes = [0, 4, 9, 10]  # Adjust as needed\n",
    "class_names = model.names\n",
    "\n",
    "# === Load test images and labels ===\n",
    "test_img_dir = Path(\"C:/Users/redoks/Documents/skripzii/data/test_ready/test/images\")\n",
    "test_label_dir = Path(\"C:/Users/redoks/Documents/skripzii/data/test_ready/test/labels\")\n",
    "\n",
    "image_paths = list(test_img_dir.glob(\"*.jpg\"))\n",
    "\n",
    "confmat = ConfusionMatrix(nc=len(class_names))  # full 12 classes\n",
    "start_time = time.time()\n",
    "\n",
    "# === Inference & Evaluation ===\n",
    "for img_path in tqdm(image_paths, desc=\"Running inference\"):\n",
    "    result = model(img_path, conf=0.489, iou=0.5, imgsz=640, device='cpu', cache=False, verbose=False)[0]\n",
    "    preds = result.boxes\n",
    "\n",
    "    # Load and filter GT\n",
    "    label_path = test_label_dir / (img_path.stem + \".txt\")\n",
    "    if not label_path.exists():\n",
    "        continue\n",
    "\n",
    "    gt_raw = []\n",
    "    with open(label_path) as f:\n",
    "        for line in f:\n",
    "            parts = list(map(float, line.strip().split()))\n",
    "            cls_id = int(parts[0])\n",
    "            if cls_id in selected_classes:\n",
    "                gt_raw.append(parts)\n",
    "\n",
    "    if not gt_raw:\n",
    "        continue\n",
    "\n",
    "    gt_raw = torch.tensor(gt_raw)\n",
    "    gt_cls = gt_raw[:, 0]\n",
    "    gt_boxes_xyxy = xywh2xyxy(gt_raw[:, 1:])  # convert normalized xywh to xyxy\n",
    "\n",
    "    # Filter prediction by selected classes\n",
    "    keep = [i for i, cls in enumerate(preds.cls) if int(cls.item()) in selected_classes]\n",
    "    if keep:\n",
    "        cls_pred = preds.cls[keep].unsqueeze(1)\n",
    "        boxes_pred = preds.xyxy[keep]\n",
    "        conf_pred = preds.conf[keep].unsqueeze(1)\n",
    "        pred_combined = torch.cat([cls_pred, boxes_pred, conf_pred], dim=1).cpu()\n",
    "    else:\n",
    "        pred_combined = torch.zeros((0, 6))\n",
    "\n",
    "    # === Use separate GT boxes and GT classes ===\n",
    "    confmat.process_batch(pred_combined, gt_boxes_xyxy, gt_cls)\n",
    "\n",
    "# === Compute results manually ===\n",
    "end_time = time.time()\n",
    "confmat_matrix = confmat.matrix  # [num_classes x num_classes] confusion matrix\n",
    "\n",
    "# Optionally print matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "# print(confmat_matrix)\n",
    "\n",
    "# === Calculate simple stats (Precision, Recall, etc) ===\n",
    "tp = confmat_matrix.diagonal()\n",
    "fp = confmat_matrix.sum(0) - tp\n",
    "fn = confmat_matrix.sum(1) - tp\n",
    "precision = tp / (tp + fp + 1e-9)\n",
    "recall = tp / (tp + fn + 1e-9)\n",
    "\n",
    "print(\"\\n=== Evaluation on Classes [0, 4, 9, 10] ===\")\n",
    "for i in selected_classes:\n",
    "    print(f\"Class {i} ({class_names[i]}): Precision {precision[i]:.4f}, Recall {recall[i]:.4f}\")\n",
    "print(f\"Inference Time: {(end_time - start_time)/len(image_paths)*1000:.2f} ms/image\")\n",
    "print(f\"Total Time:     {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7181691",
   "metadata": {},
   "source": [
    "# terminal inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1125a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference: 100%|██████████| 18/18 [00:06<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference Time: 351.20 ms/image\n",
      "Ultralytics 8.3.107  Python-3.12.0 torch-2.6.0+cpu CPU (Intel Core(TM) i5-8265U 1.60GHz)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\redoks\\Documents\\skripzii\\data\\test_ready\\test\\labels.cache... 18 images, 0 backgrounds, 0 corrupt: 100%|██████████| 18/18 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         18        104      0.917      0.131      0.148     0.0273\n",
      "               Bicycle         12         21      0.669      0.524      0.491     0.0991\n",
      "                  Boat          6         12          1          0     0.0997    0.00997\n",
      "                Bottle          6         18          1          0          0          0\n",
      "                   Bus         11         53          1          0          0          0\n",
      "Speed: 2.1ms preprocess, 228.9ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val9\u001b[0m\n",
      "\n",
      "=== mAP Metrics ===\n",
      "mAP@0.5:        0.1477\n",
      "mAP@0.5:0.95:   0.0273\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Load full model (trained with 12 classes) ===\n",
    "model_path = \"C:\\\\Users\\\\redoks\\\\Documents\\\\skripzii\\\\products6k\\\\YOLOv9\\\\YOLOv9s\\\\batch8_lr0.01\\\\weights\\\\best.pt\"\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# === Path to your updated data.yaml (should contain nc: 12 and full names list) ===\n",
    "data_yaml = \"C:/Users/redoks/Documents/skripzii/data/test_ready/data.yaml\"\n",
    "\n",
    "# === Measure Inference Time over Test Images ===\n",
    "test_img_dir = Path(\"C:/Users/redoks/Documents/skripzii/data/test_ready/test/images\")\n",
    "image_paths = list(test_img_dir.glob(\"*.jpg\"))\n",
    "\n",
    "inference_times = []\n",
    "for img_path in tqdm(image_paths, desc=\"Running inference\"):\n",
    "    start = time.time()\n",
    "    _ = model(img_path, conf=0.489, iou=0.5, imgsz=640, device='cpu', cache=False, verbose=False)\n",
    "    inference_times.append(time.time() - start)\n",
    "\n",
    "avg_time_ms = (sum(inference_times) / len(inference_times)) * 1000\n",
    "print(f\"Average Inference Time: {avg_time_ms:.2f} ms/image\")\n",
    "\n",
    "# === Compute mAP Metrics via Built-In Evaluation ===\n",
    "# This runs the evaluation on the 12-class setup as described in your data.yaml.\n",
    "val_results = model.val(data=data_yaml, split=\"test\", iou=0.5, verbose=True)\n",
    "\n",
    "# Extract and display the mAP metrics.\n",
    "print(\"\\n=== mAP Metrics ===\")\n",
    "print(f\"mAP@0.5:        {val_results.box.map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95:   {val_results.box.map:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
